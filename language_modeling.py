# -*- coding: utf-8 -*-
"""nlp1_2part.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/malaireumarianna/4897a8fd3134b7d34243164d9234b32d/nlp1_2part.ipynb
"""


'''with open('/content/NLP/TEXTEN1.txt', 'r',  encoding="iso-8859-2") as file:
    en_data = file.read().replace('\n', ' ').split()'''

en_data = [line.strip() for line in open('/content/NLP/TEXTEN1.txt', 'r', encoding="iso-8859-2")]
#len_en_data = len(en_data)
en_test_data = en_data[-20000:]
en_heldout_data = en_data[-60000:-20000]
en_train_data = en_data[:-60000]

#print(len(en_heldout_data))
#print(len(en_train_data))
#print(len(en_test_data))
#en_heldout_data

'''with open('/content/NLP/TEXTCZ1.txt', encoding="iso-8859-2" ) as file:
    cz_data = file.read().replace('\n', ' ').split()'''
cz_data = [line.strip() for line in open('/content/NLP/TEXTCZ1.txt', 'r', encoding="iso-8859-2")]
    #len_en_data = len(en_data)
cz_test_data = cz_data[-20000:]
cz_heldout_data = cz_data[-60000:-20000]
cz_train_data = cz_data[:-60000]
#" ".join(en_heldout_data)
#print(len(cz_test_data))
#print(len(cz_heldout_data))
#print(len(cz_train_data))

"""# **EN data processing**

"""

#Generate lists of n-grams from train data
from nltk.util import ngrams
vacb_en_train_data = list(set(sorted(en_train_data)))
vocab_en_train_data_size = len(vacb_en_train_data)
bigram_en_train_data = list(ngrams(en_train_data, 2))
trigram_en_train_data = list(ngrams(en_train_data, 3))
unigram_en_train_data = list(en_train_data)

#Generate lists of n-grams from heldout data
vacb_en_heldout_data = list(set(sorted(en_heldout_data)))
vocab_en_heldout_data_size = len(vacb_en_heldout_data)
bigram_en_heldout_data = list(ngrams(en_heldout_data, 2))
trigram_en_heldout_data = list(ngrams(en_heldout_data, 3))
unigram_en_heldout_data = list(en_heldout_data) 
#print(len(bigram_en_heldout_data))
#print(len(trigram_en_heldout_data))

#Generate lists of n-grams from test data
vacb_en_test_data = list(set(sorted(en_test_data)))
vocab_en_test_data_size = len(vacb_en_test_data)
bigram_en_test_data = list(ngrams(en_test_data, 2))
trigram_en_test_data = list(ngrams(en_test_data, 3))
unigram_en_test_data = list(ngrams(en_test_data, 1)) 
#print(bigram_en_test_data)

#Generate frequency on each word
def generate_tokens_freq(tokens):
    dct={}
    for i in tokens:
        dct[i]=0
    for i in tokens:
        dct[i]+=1
    return dct

dct_train=generate_tokens_freq(en_train_data)

#Bigram frequency function
def generate_ngram_freq(bigram):
    dct1={}
    for i in bigram:
        #print(i)
        st=" ".join(i)
        dct1[i]=0
    for i in bigram:
        #st=" ".join(i)
        st=" ".join(i)
        dct1[i]+=1
    return dct1

dct1_train=generate_ngram_freq(bigram_en_train_data)

dct2_train=generate_ngram_freq(trigram_en_train_data)





def unigramCount(word,unigram):
  
    if not word in dct_train.keys():
        return 0
    return dct_train[word]

def bigramCount(word, h1, bigram):
  tar_tup = (h1,word)
  res = False
  res = dct1_train.get(tar_tup) != None
  if res == False:

    return 0
  return dct1_train[tar_tup]

def trigramCount(word, h1, h2, trigram):

    tar_tup = (h2, h1,word)
    res = False
    res = dct2_train.get(tar_tup) != None
    if res == False:

      return 0
    return dct2_train[tar_tup]

def divisionOrZero(nominator, denominator):
    if denominator == 0: return 0
    return nominator / denominator

def uniformProbConditional(vocabSize):
    return 1 / vocabSize

def unigramProbConditional(word, unigram):
    return unigramCount(word, unigram) / len(unigram)

def bigramProbConditional(word, h1, unigram, bigram, vocabSize):

    if bigramCount(word, h1, bigram) == 0:
        if unigramCount(h1, unigram) == 0:
            return uniformProbConditional(vocabSize)

    return divisionOrZero(bigramCount(word, h1, bigram), unigramCount(h1, unigram))

def trigramProbConditional(word, h1, h2, unigram, bigram, trigram, vocabSize):
    if trigramCount(word, h1, h2, trigram) == 0:
        if bigramCount(h1, h2, bigram) == 0:
            return uniformProbConditional(vocabSize)
    #print(divisionOrZero(trigramCount(word, h1, h2, trigram), bigramCount(h1, h2, bigram)))
    return divisionOrZero(trigramCount(word, h1, h2, trigram), bigramCount(h1, h2, bigram))

def smoothedProbConditional(word, h1, h2, lambdas, unigram, bigram, trigram, vocabSize):
    return lambdas[0] * uniformProbConditional(vocabSize) \
           + lambdas[1] * unigramProbConditional(word, unigram) \
           + lambdas[2] * bigramProbConditional(word, h1, unigram, bigram, vocabSize) \
           + lambdas[3] * trigramProbConditional(word, h1, h2, unigram, bigram, trigram, vocabSize)

unigramCount(unigram_en_train_data[98], unigram_en_train_data)

uniform_probability_train = 1/vocab_en_train_data_size
#print(uniform_probability_train)

trigram_en_train_data[0]

tar_tup = (trigram_en_train_data[0][-1], trigram_en_train_data[0][1],trigram_en_train_data[0][0])
tar_tup

trigramCount(trigram_en_train_data[67][-1], trigram_en_train_data[67][1],trigram_en_train_data[67][0], trigram_en_train_data)

e = 0.0001
lambdas = [0.25, 0.25, 0.25,0.25]
'''lambdas[3]=0.25
lambdas[1]=0.25
lambdas[2]=0.25
lambdas[0] = 1- lambdas[3] - lambdas[2] - lambdas[1]'''

while True:
    c_l0 = 0
    c_l1 = 0
    c_l2 = 0
    c_l3 = 0


    # train lambdas using the heldoutData
    for w in trigram_en_heldout_data:
        # calculate expected counts for each lambda using uniform, unigrams, bigrams, trigrams
        c_l0 += lambdas[0] * uniformProbConditional(vocab_en_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data , vocab_en_train_data_size)
        c_l1 += lambdas[1] * unigramProbConditional(w[-1], unigram_en_train_data) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data , vocab_en_train_data_size)
        c_l2 += lambdas[2] * bigramProbConditional(w[-1], w[1], unigram_en_train_data, bigram_en_train_data, vocab_en_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size)
        c_l3 += lambdas[3] * trigramProbConditional(w[-1], w[1], w[0], unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size)

    

    l0_new =  c_l0/(c_l0 + c_l1 + c_l2 +  c_l3)
    l1_new =  c_l1/(c_l0 + c_l1 + c_l2 +  c_l3)
    l2_new =  c_l2/(c_l0 + c_l1 + c_l2 +  c_l3)
    l3_new =  c_l3/(c_l0 + c_l1 + c_l2 +  c_l3)

        
    if abs(l0_new - lambdas[0])<e and abs(l1_new - lambdas[1])<e and abs(l2_new - lambdas[2])<e and abs(l3_new - lambdas[3])<e:
        break
          #return smoothed_prob, init_l0, init_l1, init_l2, init_l3
         
        #if abs(l0_new - l0)<e and abs(l1_new - l1)<e and abs(l2_new - l2)<e and abs(l3_new - l3)<e:

    lambdas[3]=l3_new
    lambdas[1]=l1_new
    lambdas[2]=l2_new
    lambdas[0]=l0_new


#print(lambdas)

#Function which calculates cross entropy
import numpy as np
def cross_entropy(lambdas, trigram_en_test_data, dataset, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size):
    crossEntropy = 0
    for w in trigram_en_test_data:
        crossEntropy -= np.log2(smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size))  
    return crossEntropy/len(dataset)

#Calculate cross entropy of EN test data with found lambdas
cross_entropy(lambdas, trigram_en_test_data, en_test_data, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size)



#Functions for updating lambdas by busting/discounting
def modifyLambdas(lambdas, difference):
    newLambdas = list(map(lambda x: x - difference * x / sum(lambdas[0:3]), lambdas))
    newLambdas[3] = lambdas[3] + difference
    return newLambdas

def boostLambdas(lambdas, percentage):
    return modifyLambdas(lambdas, (1 - lambdas[3]) * percentage)

# Discounts the last lambda by percentage
def discountLambdas(lambdas, percentage):
    return modifyLambdas(lambdas, (-1) * lambdas[3] * (1 - percentage))


# Lists of percentages that we want to use for boosting and discounting
discountVector = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9]
boostingVector = [   .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]

#Calculate new lambdas using discounting for 3 lambda
#On each calculation modified are lambdas found while processing heldout data
#lambdas = [init_l0, init_l1, init_l2, init_l3]
discount_lambdas = []
for i in range(len(discountVector)):
    discount_lambdas.append(discountLambdas(lambdas, discountVector[i]))
discount_lambdas.append(lambdas)
#print(discount_lambdas)

#Calculate new lambdas using boosting for 3 lambda
#On each calculation modified are lambdas found while processing heldout data
boost_lambdas = []
for i in range(len(boostingVector)):
    boost_lambdas.append(boostLambdas(lambdas, boostingVector[i]))
#print(boost_lambdas)

#Calculate cross entropies for boosted lambdas
cross_entropy_test_update_boost = []
for lamb in boost_lambdas:
  cross_entropy_test_update_boost.append(cross_entropy(lamb, trigram_en_test_data, en_test_data, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size))

cross_entropy_test_update_boost

#Calculate cross entropies for discounted lambdas
cross_entropy_test_update_discount = []
for lamb in discount_lambdas:
  cross_entropy_test_update_discount.append(cross_entropy(lamb, trigram_en_test_data, en_test_data, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size))

cross_entropy_test_update_discount

"""# **CZ data processing**"""

#Generate lists of n-grams from CZ train data
vacb_cz_train_data = list(set(sorted(cz_train_data)))
vocab_cz_train_data_size = len(vacb_cz_train_data)
bigram_cz_train_data = list(ngrams(cz_train_data, 2))
trigram_cz_train_data = list(ngrams(cz_train_data, 3))
unigram_cz_train_data = list(cz_train_data)

#Generate lists of n-grams from CZ heldout data
vacb_cz_heldout_data = list(set(sorted(cz_heldout_data)))
vocab_cz_heldout_data_size = len(vacb_cz_heldout_data)
bigram_cz_heldout_data = list(ngrams(cz_heldout_data, 2))
trigram_cz_heldout_data = list(ngrams(cz_heldout_data, 3))
unigram_cz_heldout_data = list(cz_heldout_data)

#Generate lists of n-grams from CZ test data
vacb_cz_test_data = list(set(sorted(cz_test_data)))
vocab_cz_test_data_size = len(vacb_cz_test_data)
bigram_cz_test_data = list(ngrams(cz_test_data, 2))
trigram_cz_test_data = list(ngrams(cz_test_data, 3))
unigram_cz_test_data = list(cz_test_data)

#Calculates frequency of n-igrams in CZ train data
dct_train =generate_tokens_freq(cz_train_data)
dct1_train =generate_ngram_freq(bigram_cz_train_data)
dct2_train =generate_ngram_freq(trigram_cz_train_data)



e = 0.0001
lambdas = [0.25, 0.25, 0.25,0.25]
'''lambdas[3]=0.25
lambdas[1]=0.25
lambdas[2]=0.25
lambdas[0] = 1- lambdas[3] - lambdas[2] - lambdas[1]'''

while True:
    c_l0 = 0
    c_l1 = 0
    c_l2 = 0
    c_l3 = 0


    # train lambdas using the heldoutData
    for w in trigram_cz_heldout_data:
        # calculate expected counts for each lambda using uniform, unigrams, bigrams, trigrams
        c_l0 += lambdas[0] * uniformProbConditional(vocab_cz_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data , vocab_cz_train_data_size)
        c_l1 += lambdas[1] * unigramProbConditional(w[-1], unigram_cz_train_data) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data , vocab_cz_train_data_size)
        c_l2 += lambdas[2] * bigramProbConditional(w[-1], w[1], unigram_cz_train_data, bigram_cz_train_data, vocab_cz_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size)
        c_l3 += lambdas[3] * trigramProbConditional(w[-1], w[1], w[0], unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size) / smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size)

    

    l0_new =  c_l0/(c_l0 + c_l1 + c_l2 +  c_l3)
    l1_new =  c_l1/(c_l0 + c_l1 + c_l2 +  c_l3)
    l2_new =  c_l2/(c_l0 + c_l1 + c_l2 +  c_l3)
    l3_new =  c_l3/(c_l0 + c_l1 + c_l2 +  c_l3)

        
    if abs(l0_new - lambdas[0])<e and abs(l1_new - lambdas[1])<e and abs(l2_new - lambdas[2])<e and abs(l3_new - lambdas[3])<e:
        break
          #return smoothed_prob, init_l0, init_l1, init_l2, init_l3
         
        #if abs(l0_new - l0)<e and abs(l1_new - l1)<e and abs(l2_new - l2)<e and abs(l3_new - l3)<e:

    lambdas[3]=l3_new
    lambdas[1]=l1_new
    lambdas[2]=l2_new
    lambdas[0]=l0_new


#print(lambdas)

cross_entropy(lambdas, trigram_cz_test_data, cz_test_data,  unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size)

#Finding lambdas using discounting of 3 lambda

discount_lambdas_cz = []
for i in range(len(discountVector)):
    discount_lambdas_cz.append(discountLambdas(lambdas, discountVector[i]))
discount_lambdas_cz.append(lambdas)
#print(discount_lambdas_cz)

#Finding lambdas using boosting of 3 lambda
boost_lambdas_cz = []
for i in range(len(boostingVector)):
    boost_lambdas_cz.append(boostLambdas(lambdas, boostingVector[i]))

#print(boost_lambdas_cz)

#Calculate cross entropies using smoothed probabilities of test trigrams calculated with boosted lambdas
cross_entropy_test_update_boost_cz = []
for lamb in boost_lambdas_cz:
   cross_entropy_test_update_boost_cz.append(cross_entropy(lamb, trigram_cz_test_data, cz_test_data,  unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size))

cross_entropy_test_update_boost_cz

#Calculate cross entropies using smoothed probabilities of test trigrams calculated with discounted lambdas
cross_entropy_test_update_discount_cz = []
for lamb in discount_lambdas_cz:
   cross_entropy_test_update_discount_cz.append(cross_entropy(lamb, trigram_cz_test_data, cz_test_data,  unigram_cz_train_data, bigram_cz_train_data, trigram_cz_train_data,  vocab_cz_train_data_size))

cross_entropy_test_update_discount_cz

import pandas as pd
def discount_table(discountVector,discount_lambdas_cz, cross_entropy_test_update_discount_cz):
    discountVectorlist = []
    for i in discountVector:
        i = 'Discount by: ' + str(i)
        discountVectorlist.append(i)
    discountVectorlist.append('Original lambdas')


    data_table = pd.DataFrame(discountVectorlist, columns=['Discount/Boosting'])
    data_table['Cross Entropy'] = cross_entropy_test_update_discount_cz
    #data_table['Cross Entropy EN'] = cross_entropy_test_update_discount

    lambdas_cz = pd.DataFrame(discount_lambdas_cz, columns=['Lambda 0', 'Lambda 1', 'Lambda 2', 'Lambda 3'])
    data_table['Lambda 0'] = lambdas_cz['Lambda 0']
    data_table['Lambda 1'] = lambdas_cz['Lambda 1']
    data_table['Lambda 2'] = lambdas_cz['Lambda 2']
    data_table['Lambda 3'] = lambdas_cz['Lambda 3']

    return data_table

def boost_table(boostingVector,boost_lambdas_cz, cross_entropy_test_update_boost_cz):
    boostVectorlist = []
    for i in boostingVector:
        i = 'Boost by: ' + str(i)
        boostVectorlist.append(i)
    
    
    data_table1 = pd.DataFrame(boostVectorlist, columns=['Discount/Boosting'])
    data_table1['Cross Entropy'] = cross_entropy_test_update_boost_cz

    lambdas_cz = pd.DataFrame(boost_lambdas_cz, columns=['Lambda 0', 'Lambda 1', 'Lambda 2', 'Lambda 3'])
    data_table1['Lambda 0'] = lambdas_cz['Lambda 0']
    data_table1['Lambda 1'] = lambdas_cz['Lambda 1']
    data_table1['Lambda 2'] = lambdas_cz['Lambda 2']
    data_table1['Lambda 3'] = lambdas_cz['Lambda 3']

    return data_table1

def tables_union( discount_table, boost_table):
    return pd.concat([ discount_table, boost_table],  ignore_index=True)

#Tabulated results for CZ data
CZ_results_table = tables_union( discount_table(discountVector,discount_lambdas_cz, cross_entropy_test_update_discount_cz), boost_table(boostingVector,boost_lambdas_cz, cross_entropy_test_update_boost_cz))
CZ_results_table

#Tabulated results for EN data
EN_results_table =  tables_union( discount_table(discountVector,discount_lambdas, cross_entropy_test_update_discount), boost_table(boostingVector,boost_lambdas, cross_entropy_test_update_boost))
EN_results_table

"""# Plot cross entropies for EN and CZ texts

# Red line represents CZ data
# Blue line shows EN data
 
"""

import plotly.express as px

fig = px.line(EN_results_table, x='Discount/Boosting', y='Cross Entropy' )
#fig.add_scatter(CZ_results_table, x='Discount/Boosting', y='Cross Entropy', title='CZ') # Not what is desired - need a line 
fig.add_scatter(x=CZ_results_table['Discount/Boosting'], y=CZ_results_table['Cross Entropy'])

#fig = px.line(CZ_results_table, x='Discount/Boosting', y='Cross Entropy', title='CZ')
#fig.show()


  
import copy
import nltk
import numpy as np
from collections import Counter, defaultdict
from nltk import str2tuple


"""Trigram model smoothing"""
class TriEM_model:
    """Class for handling probabilities"""
    def __init__(self, tags):
        """Getting uniform, unigram, bigram, trigram probs"""
        # Get unigram counts and probs
        self.unigr_counts = Counter(tags)
        self.unigr_probs = defaultdict(float)
        unigr_N = sum(self.unigr_counts.values())
        for entry in self.unigr_counts:
            self.unigr_probs[entry] = float(self.unigr_counts[entry]) / unigr_N

        # Get bigram counts and probs
        self.bigr_counts = Counter(nltk.bigrams(tags))
        self.bigr_probs = defaultdict(float)
        for entry in self.bigr_counts:
            self.bigr_probs[entry] = float(self.bigr_counts[entry]) / self.unigr_counts[entry[0]]

        # Get trigram counts and probs
        self.trigr_counts = Counter(nltk.trigrams(tags))
        self.trigr_probs = defaultdict(float)
        for entry in self.trigr_counts:
            self.trigr_probs[entry] = float(self.trigr_counts[entry]) / self.bigr_counts[(entry[0], entry[1])]

        # Get uniform probability
        self.unif_prob = 1. / len(self.unigr_counts)

    def get_probs(self, hist2, hist1, word):
        """Getting probabilities for a word"""
        # Get probability for a word. If not present in the data, prob == 0.0
        p1 = self.unigr_probs[word]
        p2 = self.bigr_probs[(hist1, word)]
        p3 = self.trigr_probs[(hist2, hist1, word)]
        # Assign uniform prob when history for a word is unknown
        if p2 == 0.:
            p2 = 1. / len(self.unigr_probs) if self.unigr_probs[hist1] == 0. else 0.
        if p3 == 0.:
            p3 = 1. / len(self.unigr_probs) if self.bigr_probs[(hist2, hist1)] == 0. else 0.
        return self.unif_prob, p1, p2, p3

    def EM(self, H):
        """Smoothing EM algorithm: obtain lambdas"""
        # Initialize probability dictionaries
        self.lambdas = [0.25, 0.25, 0.25, 0.25]    # initial lambdas
        expected_lambdas = [0., 0., 0., 0.]
        old_lambdas = [0., 0., 0., 0.]
        # While changes of lambdas are significant
        while all(el > 0.00001 for el in np.absolute(np.subtract(old_lambdas, self.lambdas))):
            old_lambdas = copy.deepcopy(self.lambdas)
            # Create histories
            hist1 = "###"
            hist2 = "###"
            for word in H:
                p0, p1, p2, p3 = self.get_probs(hist2, hist1, word)
                # Compute smoothed prob
                p_lambda = self.lambdas[0] * p0 + self.lambdas[1] * p1 + self.lambdas[2] * p2 + self.lambdas[3] * p3
                # Update histories
                hist2 = hist1
                hist1 = word
                # Compute expected counts of lambdas
                expected_lambdas[0] += (self.lambdas[0] * p0 / p_lambda)
                expected_lambdas[1] += (self.lambdas[1] * p1 / p_lambda)
                expected_lambdas[2] += (self.lambdas[2] * p2 / p_lambda)
                expected_lambdas[3] += (self.lambdas[3] * p3 / p_lambda)
            # Recompute lambdas
            self.lambdas = [el / sum(expected_lambdas) for el in expected_lambdas]

    def trigr_smooth(self, data):
        """Smoothing of the whole language model using computed lambdas"""
        p_smoothed = defaultdict(float)
        # Create histories
        hist1 = "###"
        hist2 = "###"
        for word in data:
            p0, p1, p2, p3 = self.get_probs(hist2, hist1, word)
            # Compute smoothed prob
            p_lambda = self.lambdas[0] * p0 + self.lambdas[1] * p1 + self.lambdas[2] * p2 + self.lambdas[3] * p3
            # Rewrite probabilities
            p_smoothed[(hist2, hist1, word)] = p_lambda
            # Update histories
            hist2 = hist1
            hist1 = word
        return p_smoothed

    def trans_probs(self, hist2, hist1, word):

        p0, p1, p2, p3 = self.get_probs(hist2, hist1, word)
        p_lambda = self.lambdas[0] * p0 + self.lambdas[1] * p1 + self.lambdas[2] * p2 + self.lambdas[3] * p3
        return p_lambda


"""Lexical model and unigram model smoothing"""
class Lexical_model_smooth:

    def __init__(self, data):
        words, tags = data[0], data[1]
        self.w_t_counts = Counter([(words[i], tags[i]) for i in range(len(words) - 1)])
        self.t_counts = Counter(tags)
        self.words, self.tags = list(set(words)), list(set(tags))
        self.a = 2 ** (-20)
        self.V = len(self.words) * len(self.tags)   # vocabulary size
        self.N = len(words)        # data size

    def get_probs(self, word, tag):
        if (word, tag) in self.w_t_counts:
            return self.w_t_counts[(word, tag)] / self.t_counts[tag]
        return 1. / self.V

    def lex_smooth(self, data):
        p_smoothed = defaultdict(float)
        for token in data:
            word, tag = str2tuple(token)
            p_smoothed[(word, tag)] = self.get_probs(word, tag)
        return p_smoothed

    def emis_probs(self, word, tag):
        return self.get_probs(word, tag)

    def emis_probs_BW(self, word, tag):
        if (tag, word) in self.w_t_counts:
            return self.w_t_counts[(tag, word)]
        return 1. / self.V


class Unigram_model_smooth(Lexical_model_smooth):
    def get_probs(self, word, tag):
        """Getting probabilities for a tag"""
        return (self.t_counts[tag] + self.a) / (self.N + self.a * self.V)

    def init_probs(self, word, tag):
        """Getting an initial probability for Viterbi decoding"""
        return self.get_probs(word, tag)


#Function which calculates cross entropy
import numpy as np
def cross_entropy(lambdas, trigram_en_test_data, dataset, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size):
    crossEntropy = 0
    for w in trigram_en_test_data:
        crossEntropy -= np.log2(smoothedProbConditional(w[-1], w[1], w[0], lambdas, unigram_en_train_data, bigram_en_train_data, trigram_en_train_data,  vocab_en_train_data_size))  
    return crossEntropy/len(dataset)
